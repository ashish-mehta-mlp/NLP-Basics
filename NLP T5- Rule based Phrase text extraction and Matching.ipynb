{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy-lookups-data\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/f1/be61b032e02a06a221e14f906dc251de90ac459dc2739f0c5225844ecb08/spacy_lookups_data-0.2.0.tar.gz (29.2MB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in f:\\anaconda3\\lib\\site-packages (from spacy-lookups-data) (40.8.0)\n",
      "Building wheels for collected packages: spacy-lookups-data\n",
      "  Building wheel for spacy-lookups-data (setup.py): started\n",
      "  Building wheel for spacy-lookups-data (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Ashish\\AppData\\Local\\pip\\Cache\\wheels\\79\\a4\\b8\\6085d282396938b29675292697e72871b145990d0079ceadc1\n",
      "Successfully built spacy-lookups-data\n",
      "Installing collected packages: spacy-lookups-data\n",
      "Successfully installed spacy-lookups-data-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy-lookups-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In machine learning, there are two types of models:\n",
    "    1. Statistical model \n",
    "    2. Rule based model\n",
    "    \n",
    "- Statistical models are models which are already trained with alot of data and we just have to implement them for getting our analysis done.\n",
    "- But sometimes we have to deal with data that cannot be handled using statistical model. That is where Rule based model comes into picture.\n",
    "- In rule based model, we set our own rules in order to analyze the data.\n",
    "- In NLP, rule based matching can be done using regular expressions and by using Spacy's Rule based matcher.\n",
    "- Spacys rule based matcher is more flexible. They let you find the words and phrases you are looking for and they also give you access to the tokens within the documents and their relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is also token based matching in which we convert the documents into tokens and then apply matching rules on it.\n",
    "- This means, if a token matches with the mentioned rule, then we can extract that token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello World!"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Token Based Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression\n",
    "\n",
    "- In token based matching, we try to match and find the exact string. But in most cases, we have to find the similar type of string and not the exact one.\n",
    "- Eg: How many times did YouTube occured in a doc? Or how many times did Ashish Mehta occur in a document.\n",
    "- For the above example, we can do hard coding ie Token based Matching.\n",
    "- But for eg: We are looking for all the three character words in a document [dog, mat, bat, sat]. These words can be anything\n",
    "- For extracting these three chracter words, we have to define a generalized Rule which can look for 3 character words in a document and extract them.\n",
    "- This can be done by using Regular Expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta Characters in Regular Expressions:\n",
    "\n",
    "- []    - Matches any character contained between the [] brackets\n",
    "- [^ ]  - Matches any character that is not contained between the [] brackets\n",
    "- [*]    - Matches zero or more repetitions of preceeding symbols\n",
    "- [+]    - Matches one or more repetitions of preceeding symbols\n",
    "- ?     - Makes the preceeding symbol optional.\n",
    "- {n,m} - Braces. Matches n but not more than m reprtitions of the preceeding symbols\n",
    "- (xyz) - Character group. Matches the character xyz in the exact order.\n",
    "- |     - Alternation. Matches either the characters before or characters after the symbol\n",
    "- \\     - Escapes the next character. This allows you to match reserved characters [] () {} . * + ? ^ $ \\ |.\n",
    "- ^     - Matches the beginning of the input\n",
    "- $     - Matches the end of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifiers And Quantifiers are very necessary in Regular expressions\n",
    "\n",
    "- Look for Identifiers and Quantifiers in Regular expressions online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My name is Ashish. My number is 1562. Ohh, thats a wrong number! My correct Number is 9900748547\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regular expression\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(86, 96), match='9900748547'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all 10 digit number in the text\n",
    "re.search(r\"\\d{10}\", text)\n",
    "\n",
    "# \\d means digit\n",
    "# {10} is a quantifier to find 10 digit number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(32, 36), match='1562'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all 4 digit number in text\n",
    "re.search(r\"\\d{4}\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1562', '9900748547']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all the numbers in text which have more than 3 digits but less than 4 digits\n",
    "\n",
    "re.findall(r\"\\d{3,10}\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To find more than one result, we make use of **findall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'Ashi',\n",
       " 'numb',\n",
       " '1562',\n",
       " 'that',\n",
       " 'wron',\n",
       " 'numb',\n",
       " 'corr',\n",
       " 'Numb',\n",
       " '9900',\n",
       " '7485']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all the words which have ATLEAST 4 characters\n",
    "\n",
    "re.findall(r\"\\w{4}\", text) # over here, we can see that some of the words are trunkated. [Cut-short]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'Ashish',\n",
       " 'number',\n",
       " '1562',\n",
       " 'thats',\n",
       " 'wrong',\n",
       " 'number',\n",
       " 'correct',\n",
       " 'Number',\n",
       " '9900748547']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To deal with trunkation, we make use of ,\n",
    "\n",
    "re.findall(r\"\\w{4,}\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WildCard Text\n",
    "\n",
    "- We want to find a word that starts with letter n. How do we do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name i', 'number', 'ng num']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"n.....\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- be careful with the number of dots you are putting in the bracket as only those many characters of the word will be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ashish']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"Ashish\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new text\n",
    "\n",
    "text = \"This is a cat but not that. I want a cat and a hat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' a ', 'cat', 'hat', 'wan', ' a ', 'cat', ' an', ' a ', 'hat']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all three letter words that has a in between\n",
    "\n",
    "re.findall(r\".a.\", text) # white space also counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a number at the end of the text\n",
    "\n",
    "text = \"Thanks for subscribing <3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\d$\", text) # This will only work if the digit is at the absolute end of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above code means, this string ends with a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"9 Thanks for subscribing <3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"^\\d\", text) # This means, the string starts with a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclusion in Regular Expression\n",
    "\n",
    "- Exclusion means extracting the data by excluding something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets say we have to extract all the text but without the numbers\n",
    "\n",
    "text = \"9 Thanks for subscribing <3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ^ sign inside [] bracket indicates negate operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'T',\n",
       " 'h',\n",
       " 'a',\n",
       " 'n',\n",
       " 'k',\n",
       " 's',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 's',\n",
       " 'u',\n",
       " 'b',\n",
       " 's',\n",
       " 'c',\n",
       " 'r',\n",
       " 'i',\n",
       " 'b',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " '<']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[^\\d]\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Thanks for subscribing <']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want the sentence without the numbers\n",
    "\n",
    "re.findall(r\"[^\\d]+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9', '3']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to get all the numbers in the text\n",
    "\n",
    "re.findall(r\"[^\\D]+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to extract all words with - sign\n",
    "\n",
    "text = \"Thanks for Ashish-Mehta. I am The-Best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ashish-Mehta', 'The-Best']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[\\w]+-+[\\w]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 3 + signs. The last + sign is for combining all the letters of the word together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression in Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Google announced a new pixel at Google I/O. Google I/O is a great place to get all updates from Google.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this text we have to find how many times has Google I/O appeared in the sentence.\n",
    "\n",
    "pattern = [{\"TEXT\":\"Google\"}, {\"TEXT\":\"I\"}, {\"TEXT\":\"/\"}, {\"TEXT\":\"O\"},{\"IS_PUNCT\":True, \"OP\":\"?\"}]\n",
    "\n",
    "# \"OP\":\"?\" means print when there is punctuation and when there is no punctuation with Google I/O "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a callback function, to call the output once the match is found\n",
    "\n",
    "def callback (matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = doc[start:end]\n",
    "    print(entity.text)\n",
    "    \n",
    "# Here, match_id is the id of the match found in the text\n",
    "# start, end is from where the doc will start and end. doc[start:end] this describes the entire span of doc from start to end\n",
    "# doc is the document on which the matcher is run\n",
    "# matches[i] stands for number of matches and i stands for index of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"Google\", callback, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google I/O\n",
      "Google I/O .\n",
      "Google I/O\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(11578853341595296054, 6, 10),\n",
       " (11578853341595296054, 6, 11),\n",
       " (11578853341595296054, 11, 15)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic Annotations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets say you are analyzing your comments and you want to find out, what people are saying about facebook. You want to start off by finding adjectives following \"Facebook is\" or \"Facebook was\". This is a very rudimentary solution, but it will be fast, and a great way to get an idea for whats in your data. Your pattern could look like this.\n",
    "\n",
    "**pattern = [{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"}, {\"POS\": \"ADJ\"}]**\n",
    "\n",
    "- This translates to a token whose lowercase form matches Facebook (like Facebook, facebook or FACEBOOK), followed by a token with a lemma be (is,was,or) followed by an optional adverb followed by an adjective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linguistic annotations can be used for sentimental analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Annotations are available on: https://spacy.io/api/annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"}, {\"POS\": \"ADJ\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback (matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i] # One match returns a tuple that contains match_id, start and end\n",
    "    span = doc[start:end] # entire document from start to end\n",
    "    sent = span.sent\n",
    "    \n",
    "    match_ents = [{                                     # Created our own entity. It will highlight if a match is found  \n",
    "        \"start\": span.start_char - sent.start_char,\n",
    "        \"end\": span.end_char - sent.start_char,\n",
    "        \"label\": \"MATCH\"\n",
    "    }]\n",
    "    \n",
    "    matched_sents.append({\"text\": sent.text, \"ents\":match_ents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"fb\", callback ,pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I'd say that facebook is evil. - Facebook is pretty cool, right?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8017838677478259815, 4, 7), (8017838677478259815, 9, 13)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"I'd say that facebook is evil.\",\n",
       "  'ents': [{'start': 13, 'end': 29, 'label': 'MATCH'}]},\n",
       " {'text': '- Facebook is pretty cool, right?',\n",
       "  'ents': [{'start': 2, 'end': 25, 'label': 'MATCH'}]}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I'd say that \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    facebook is evil\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MATCH</span>\n",
       "</mark>\n",
       ".</div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">- \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Facebook is pretty cool\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MATCH</span>\n",
       "</mark>\n",
       ", right?</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(matched_sents, style = \"ent\", manual = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Phone Number\n",
    "\n",
    "**Phone numbers can have many different formats and matching them can often be very tricky. During Tokenization, Spacy will leave sequences of numbers intact and only split on whitespace and punctuations. This means that your match patterns will have to look out for number sequences of a certain length, surrounded by specific punctuations - depending on the national conventions**\n",
    "\n",
    "**You want to match like this: (123) 4567 8901 or (123) 4567-8901**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **pattern = [{\"ORTH\":\"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\":\")\"}, {\"SHAPE\":\"dddd\"}, {\"ORTH\":\"-\", \"OP\":\"?\"}, {\"SHAPE\":\"dddd\"}]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"ORTH\":\"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\":\")\"}, {\"SHAPE\":\"dddd\"}, {\"ORTH\":\"-\", \"OP\":\"?\"}, {\"SHAPE\":\"dddd\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"PhoneNumber\", None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Call me at (123) 4857 6254 or call me at (154) 5874-5625\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Call', 'me', 'at', '(', '123', ')', '4857', '6254', 'or', 'call', 'me', 'at', '(', '154', ')', '5874', '-', '5625']\n"
     ]
    }
   ],
   "source": [
    "# tokenizing the sentence\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7978097794922043545, 3, 8), (7978097794922043545, 12, 18)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123) 4857 6254\n",
      "(154) 5874-5625\n"
     ]
    }
   ],
   "source": [
    "# printing the actual numbers in the sentence\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting email addresses from a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": {\"REGEX\": \"[a-zA-Z0-9-_.]+@[a-zA-Z0-9.]+\"}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"please msg me at xyz@gmail.com or at ags_123@hotmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"Email\", None, pattern) #\"Email\" is the name of matcher, Callback function is none, pattern is the mentioned pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11010771136823990775, 4, 5), (11010771136823990775, 7, 8)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xyz@gmail.com\n",
      "ags_123@hotmail.com\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags and Emoji Detection and Extraction on social media\n",
    "\n",
    "**Social media posts, especially tweets, can be difficult to work with. They're very short and often contain various emojis and hashtags. By only looking at the plain text, we will lose alot of valuable semantic information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By looking at hashtags and emojis, we can detect the sentiment of a post\n",
    "- Eg: If a person has posted a post with alot of laughing emojis, that means the post sentiment is happy. Whereas if there are alot of crying emojis, then the sentiment of the post is sad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in f:\\anaconda3\\lib\\site-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\":\"#\"}, {\"IS_ASCII\": True}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"#Sadak ekdum kadak. I love myself. #Happylife\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"Hashtag\", None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Sadak\n",
      "#Happylife\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Phrase Matching\n",
    "\n",
    "**If you want to match large terminology list, we can also use the PhraseMatcher and create Doc objects instead of token patterns, which is much more efficient overall. The doc patterns can contain single or multiple tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"BARAK OBAMA\", \"ANGELA MERKEL\", \"WASHINGTON D.C\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have to get these terms from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [nlp.make_doc(text) for text in terms]\n",
    "\n",
    "# for each value in terms, search for the value in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BARAK OBAMA, ANGELA MERKEL, WASHINGTON D.C]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"term\", None, *pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"German Chancellor ANGELA MERKEL and US President BARAK OBAMA had a mmeting about world economy at WASHINGTON D.C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "German Chancellor ANGELA MERKEL and US President BARAK OBAMA had a mmeting about world economy at WASHINGTON D.C"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANGELA MERKEL\n",
      "BARAK OBAMA\n",
      "WASHINGTON D.C\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4519742297340331040, 2, 4),\n",
       " (4519742297340331040, 7, 9),\n",
       " (4519742297340331040, 16, 18)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Rule Based Entity Recognition\n",
    "\n",
    "The Entity Ruler is an exciting new component that lets you add named entities based on pattern dictionaries, and makes it easy to combine rule-based and statistical named entity recognition for even more powerful models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity Patterns\n",
    "\n",
    "**Entity patterns are dictionaries with two keys: \"label\", specifying the label to assign to the entity if the pattern is matched, and \"pattern\", the match pattern. The entity ruler accepts two types of patterns:**\n",
    "        \n",
    "    1. PHRASE PATTERN: {\"label\":\"ORG\", \"pattern\": \"KGP talkie\"}\n",
    "    2. TOKEN PATTERN: {\"label\":\"ORG\", \"pattern\": [{\"LOWER\":\"san\"},{\"LOWER\":\"fransisco\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Entity Ruler is a pipeline comment that's typically added via nlp.add_pipe. When the nlp object is called on a text, it will find matches in the doc and add them as entities to the doc.ents, using the specified pattern label as the entity label.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = EntityRuler(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pattern for KGP talkie\n",
    "\n",
    "pattern = [{\"label\":\"ORG\", \"pattern\": \"KGP talkie\"},\n",
    "           {\"label\":\"GPE\", \"pattern\": [{\"LOWER\":\"san\"},{\"LOWER\":\"fransisco\"}]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'ORG', 'pattern': 'KGP talkie'},\n",
       " {'label': 'GPE', 'pattern': [{'LOWER': 'san'}, {'LOWER': 'fransisco'}]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(ruler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"KGP talkie is opening its first big office in San Fransisco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KGP talkie is opening its first big office in San Fransisco"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KGP ORG\n",
      "first ORDINAL\n",
      "San Fransisco GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
